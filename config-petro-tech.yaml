
logger:
    console:
        level: INFO

# Cognite project to stream your datapoints from
cognite:
    host: ${COGNITE_BASE_URL}
    project: ${COGNITE_PROJECT}

    idp-authentication:
        token-url: ${COGNITE_TOKEN_URL}
        client-id: ${COGNITE_CLIENT_ID}
        secret: ${COGNITE_CLIENT_SECRET}
        scopes:
            - ${COGNITE_BASE_URL}/.default
    extraction-pipeline:
        external-id: fabric-replicator

#Extractor config
extractor:
    state-store:
        local:
            path: state.json
    subscription-batch-size: 10000
    ingest-batch-size: 100000
    poll-time: 5


# subscriptions to stream
subscription:
  external_id: gauge-ts
  name: Gauge Timeseries
  num_partitions: 1
  lakehouse_abfss_path_dps: abfss://a0fd605a-3a69-401e-a586-bee3bf2c8127@onelake.dfs.fabric.microsoft.com/40b75c4d-5ddd-47ff-bd81-8faadec1804b/Tables/Timeseries
  lakehouse_abfss_path_ts: abfss://a0fd605a-3a69-401e-a586-bee3bf2c8127@onelake.dfs.fabric.microsoft.com/40b75c4d-5ddd-47ff-bd81-8faadec1804b/Tables/TimeseriesMetadata


# sync data model
#data_modeling:
#  - space: cc_plant
#    lakehouse_abfss_prefix: abfss://FabricCogniteDemo@onelake.dfs.fabric.microsoft.com/CogniteLakehouse.Lakehouse
